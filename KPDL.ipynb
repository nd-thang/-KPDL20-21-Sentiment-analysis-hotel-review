{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "KPDL.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNBzBpH2m8wuuNECqCr7xWN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nd-thang/-KPDL20-21-Sentiment-analysis-hotel-review/blob/main/KPDL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wcYk_-s5C94W",
        "outputId": "699b6b41-92ef-4790-de1a-b04557a532f0"
      },
      "source": [
        "!git clone https://github.com/nd-thang/-KPDL20-21-Sentiment-analysis-hotel-review.git data"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'data'...\n",
            "remote: Enumerating objects: 10, done.\u001b[K\n",
            "remote: Counting objects: 100% (10/10), done.\u001b[K\n",
            "remote: Compressing objects: 100% (10/10), done.\u001b[K\n",
            "remote: Total 10 (delta 1), reused 0 (delta 0), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (10/10), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0WAJbWIwfLYq"
      },
      "source": [
        "###Count each label"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tn3QdCDFfKD0",
        "outputId": "a0ec8d27-8ba6-442e-d124-e8dc1bd56833"
      },
      "source": [
        "filename = '/data/sentiment_analysis_train.v1.0.txt'\n",
        "f = open(filename, \"r\", encoding=\"utf8\")\n",
        "lines = f.readlines()\n",
        "labels = []\n",
        "reviews =[]\n",
        "for line in lines:\n",
        "    l = line.strip().split(' ', 1)\n",
        "    labels.append(l[0])\n",
        "    reviews.append(l[1])\n",
        "\n",
        "print(len(labels))\n",
        "print(len(reviews))\n",
        "\n",
        "print(labels.count(''))\n",
        "print(labels.count(' '))\n",
        "print(reviews.count(''))\n",
        "print(reviews.count(' '))\n",
        "for label in set(labels):\n",
        "    print(labels.count(label))\n",
        "    print(label)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "22999\n",
            "22999\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "214\n",
            "__label__rat_kem\n",
            "10735\n",
            "__label__tot\n",
            "4569\n",
            "__label__trung_binh\n",
            "6396\n",
            "__label__xuat_sac\n",
            "1085\n",
            "__label__kem\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qeI-hYNufdQw"
      },
      "source": [
        "###Translate train and test file to lower english"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xQfo4sGdMeAk",
        "outputId": "64895c63-3227-44ee-9020-1e79417d33f1"
      },
      "source": [
        "pip install google-trans-new"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting google-trans-new\n",
            "  Downloading https://files.pythonhosted.org/packages/f9/7b/9f136106dc5824dc98185c97991d3cd9b53e70a197154dd49f7b899128f6/google_trans_new-1.1.9-py3-none-any.whl\n",
            "Installing collected packages: google-trans-new\n",
            "Successfully installed google-trans-new-1.1.9\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GVyuGqjfMjK-",
        "outputId": "32ea2a08-b780-400d-85d5-73144d30822d"
      },
      "source": [
        "from google_trans_new import google_translator  \n",
        "\n",
        "translator = google_translator()\n",
        "print(translator.translate('anh yeu em'))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "I love you \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vcZg7lSMM1y3"
      },
      "source": [
        "#file train\n",
        "from google_trans_new import google_translator\n",
        "f = open(\"/data/en_train_fin_lower.txt\", \"w\")\n",
        "translator = google_translator()\n",
        "filename = '/data/sentiment_analysis_train.v1.0.txt'\n",
        "f2 = open(filename, \"r\", encoding=\"utf8\")\n",
        "lines = f2.readlines()\n",
        "#lines2 = lines[:10]\n",
        "for line in lines:\n",
        "  l = line.strip().split(' ', 1)\n",
        "  f.write(l[0] + ' ' + translator.translate(l[1]).lower() + \"\\n\")\n",
        "  #f2.write(translator.translate(line).lower() + \"\\n\")\n",
        "f2.close()\n",
        "f.close()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VgQADkKrHC1R"
      },
      "source": [
        "#file test\n",
        "f = open(\"/data/en_test_fin_lower.txt\", \"w\")\n",
        "filename = '/data/sentiment_analysis_test_unlabel.v1.0.txt'\n",
        "f2 = open(filename, \"r\", encoding=\"utf8\")\n",
        "lines = f2.readlines()\n",
        "#lines2 = lines[:10]\n",
        "for line in lines:\n",
        "  f.write(translator.translate(line).lower() + \"\\n\")\n",
        "f2.close()\n",
        "f.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1z2l8jyAqKzK"
      },
      "source": [
        "###Create labels, reviews, tests lists"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kHAH_te9ACP5"
      },
      "source": [
        "filename = \"/data/en_train_fin_lower.txt\"\n",
        "f = open(filename, \"r\", encoding=\"utf8\")\n",
        "lines = f.readlines()\n",
        "labels = []\n",
        "reviews =[]\n",
        "for line in lines:\n",
        "    l = line.strip().split(' ', 1)\n",
        "    labels.append(l[0])\n",
        "    reviews.append(l[1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sL47kSPHpqyw"
      },
      "source": [
        "filename = '/data/en_test_fin_lower.txt'\n",
        "f = open(filename, \"r\", encoding=\"utf8\")\n",
        "lines = f.readlines()\n",
        "tests =[]\n",
        "for line in lines:\n",
        "    tests.append(line)\n",
        "f.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TSIYYEK1AHMz",
        "outputId": "42018b03-6617-4a17-9fdd-945d8b1912a0"
      },
      "source": [
        "pip install sklearn"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: sklearn in /usr/local/lib/python3.6/dist-packages (0.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from sklearn) (0.22.2.post1)\n",
            "Requirement already satisfied: numpy>=1.11.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->sklearn) (1.18.5)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->sklearn) (1.4.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->sklearn) (0.17.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AQGcH4mEBWhr"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(reviews, labels, test_size=0.2, random_state=19)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mRFko2Aeq543"
      },
      "source": [
        "###LogisticRegression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JLHdfh7tBwf0",
        "outputId": "49400634-4024-4224-bc6d-4ed6eea47c81"
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "import time\n",
        "start_time = time.time()\n",
        "text_clf = Pipeline([('vect', CountVectorizer(ngram_range=(1,1),\n",
        "                                             max_df=0.8,\n",
        "                                             max_features=None)), \n",
        "                     ('tfidf', TfidfTransformer()),\n",
        "                     ('clf', LogisticRegression(solver='lbfgs', \n",
        "                                                multi_class='auto',\n",
        "                                                max_iter=10000))\n",
        "                    ])\n",
        "text_clf = text_clf.fit(X_train, y_train)\n",
        " \n",
        "train_time = time.time() - start_time\n",
        "print('Done training Linear Classifier in', train_time, 'seconds.')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Done training Linear Classifier in 6.67122220993042 seconds.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rcpqZQ_HB1iP",
        "outputId": "17672258-abf3-4b14-87b4-689e8b7d9a39"
      },
      "source": [
        "y_pred = text_clf.predict(X_test)\n",
        "text_clf.score(X_test, y_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.6704347826086956"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-E2uSVd-C56U",
        "outputId": "8fa01371-4b26-4b7c-9291-6d6f43c2091d"
      },
      "source": [
        "print(len(y_pred))\n",
        "print(len(X_test))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4600\n",
            "4600\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lmrgemuv5cR6",
        "outputId": "afb08eda-8902-47b6-e9f2-fab48a012f40"
      },
      "source": [
        "#kfold LR\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "import time\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.model_selection import KFold\n",
        "\n",
        "scores = []\n",
        "kf = KFold(n_splits=5, shuffle=True)\n",
        "np_reviews = np.array(reviews)\n",
        "np_labels = np.array(labels)\n",
        "for train_index, test_index in kf.split(np_reviews):\n",
        "    X_train, X_test = np_reviews[train_index], np_reviews[test_index]\n",
        "    y_train, y_test = np_labels[train_index], np_labels[test_index]\n",
        "    model = Pipeline([('vect', CountVectorizer(ngram_range=(1,1),\n",
        "                                             max_df=0.8,\n",
        "                                             max_features=None)), \n",
        "                     ('tfidf', TfidfTransformer()),\n",
        "                     ('clf', LogisticRegression(solver='lbfgs', \n",
        "                                                multi_class='auto',\n",
        "                                                max_iter=10000))\n",
        "                    ])\n",
        "    model.fit(X_train, y_train)\n",
        "    scores.append(model.score(X_test, y_test))\n",
        "print(scores)\n",
        "print(np.mean(scores))\n",
        "final_model = Pipeline([('vect', CountVectorizer(ngram_range=(1,1),\n",
        "                                             max_df=0.8,\n",
        "                                             max_features=None)), \n",
        "                     ('tfidf', TfidfTransformer()),\n",
        "                     ('clf', LogisticRegression(solver='lbfgs', \n",
        "                                                multi_class='auto',\n",
        "                                                max_iter=10000))\n",
        "                    ])\n",
        "final_model.fit(np_reviews, np_labels)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.6617391304347826, 0.6689130434782609, 0.6702173913043479, 0.6682608695652174, 0.6642748423570342]\n",
            "0.6666810554279285\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Pipeline(memory=None,\n",
              "         steps=[('vect',\n",
              "                 CountVectorizer(analyzer='word', binary=False,\n",
              "                                 decode_error='strict',\n",
              "                                 dtype=<class 'numpy.int64'>, encoding='utf-8',\n",
              "                                 input='content', lowercase=True, max_df=0.8,\n",
              "                                 max_features=None, min_df=1,\n",
              "                                 ngram_range=(1, 1), preprocessor=None,\n",
              "                                 stop_words=None, strip_accents=None,\n",
              "                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
              "                                 tokenizer=None, vocabulary=None)),\n",
              "                ('tfidf',\n",
              "                 TfidfTransformer(norm='l2', smooth_idf=True,\n",
              "                                  sublinear_tf=False, use_idf=True)),\n",
              "                ('clf',\n",
              "                 LogisticRegression(C=1.0, class_weight=None, dual=False,\n",
              "                                    fit_intercept=True, intercept_scaling=1,\n",
              "                                    l1_ratio=None, max_iter=10000,\n",
              "                                    multi_class='auto', n_jobs=None,\n",
              "                                    penalty='l2', random_state=None,\n",
              "                                    solver='lbfgs', tol=0.0001, verbose=0,\n",
              "                                    warm_start=False))],\n",
              "         verbose=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TtGUdG1vrAXG"
      },
      "source": [
        "###SVM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IQT9gccFDUZp",
        "outputId": "a351d452-d7eb-46b0-c99b-ce774e6562d4"
      },
      "source": [
        "from sklearn.svm import SVC\n",
        " \n",
        "start_time = time.time()\n",
        "text_clf = Pipeline([('vect', CountVectorizer(ngram_range=(1,1),\n",
        "                                             max_df=0.8,\n",
        "                                             max_features=None)), \n",
        "                     ('tfidf', TfidfTransformer()),\n",
        "                     ('clf', SVC(gamma='scale'))\n",
        "                    ])\n",
        "text_clf = text_clf.fit(X_train, y_train)\n",
        " \n",
        "train_time = time.time() - start_time\n",
        "print('Done training SVM in', train_time, 'seconds.')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Done training SVM in 150.69980096817017 seconds.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GpLT3PxREOxJ",
        "outputId": "214788b0-b3fd-4d51-d816-43ddcae52c67"
      },
      "source": [
        "y_pred = text_clf.predict(X_test)\n",
        "text_clf.score(X_test, y_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.6743478260869565"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BzICTm7Y69wj",
        "outputId": "4bf31535-dc20-4ae7-f4f5-bd090c88b421"
      },
      "source": [
        "#svm kfold\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "import time\n",
        "import numpy as np\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "from sklearn.model_selection import KFold\n",
        "\n",
        "scores = []\n",
        "kf = KFold(n_splits=5, shuffle=True)\n",
        "np_reviews = np.array(reviews)\n",
        "np_labels = np.array(labels)\n",
        "for train_index, test_index in kf.split(np_reviews):\n",
        "    X_train, X_test = np_reviews[train_index], np_reviews[test_index]\n",
        "    y_train, y_test = np_labels[train_index], np_labels[test_index]\n",
        "    model = Pipeline([('vect', CountVectorizer(ngram_range=(1,1),\n",
        "                                             max_df=0.8,\n",
        "                                             max_features=None)), \n",
        "                     ('tfidf', TfidfTransformer()),\n",
        "                     ('clf', SVC(gamma='scale'))\n",
        "                    ])\n",
        "    model.fit(X_train, y_train)\n",
        "    scores.append(model.score(X_test, y_test))\n",
        "print(scores)\n",
        "print(np.mean(scores))\n",
        "final_model = Pipeline([('vect', CountVectorizer(ngram_range=(1,1),\n",
        "                                             max_df=0.8,\n",
        "                                             max_features=None)), \n",
        "                     ('tfidf', TfidfTransformer()),\n",
        "                     ('clf', SVC(gamma='scale'))\n",
        "                    ])\n",
        "#final_model.fit(np_reviews, np_labels)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.6797826086956522, 0.672608695652174, 0.6580434782608696, 0.6680434782608695, 0.6677538595346815]\n",
            "0.6692464240808493\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-UsLVv4OrXIf"
      },
      "source": [
        "###Naive bayes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kiPvErsuElpk",
        "outputId": "7b5344dd-156f-47a1-e00c-7ffa4e362b7a"
      },
      "source": [
        "#naive bayes\n",
        "start_time = time.time()\n",
        "text_clf = Pipeline([('vect', CountVectorizer(ngram_range=(1,1),\n",
        "                                             max_df=0.8,\n",
        "                                             max_features=None)), \n",
        "                     ('tfidf', TfidfTransformer()), \n",
        "                     ('clf', MultinomialNB())\n",
        "                    ])\n",
        "text_clf = text_clf.fit(X_train, y_train)\n",
        " \n",
        "train_time = time.time() - start_time\n",
        "print('Done training Naive Bayes in', train_time, 'seconds.')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Done training Naive Bayes in 0.8197929859161377 seconds.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8oT92ZvSEsLA",
        "outputId": "4f67fa18-8bc5-4b9d-e0dd-bafe808bab5f"
      },
      "source": [
        "y_pred = text_clf.predict(X_test)\n",
        "text_clf.score(X_test, y_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.5927375516416612"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OfdHJcQL7PsM",
        "outputId": "eb0f9ab5-8157-4c19-bdff-947a7e4053e0"
      },
      "source": [
        "#naive bayes kfold\n",
        "#kfold LR\n",
        "scores = []\n",
        "kf = KFold(n_splits=5, shuffle=True)\n",
        "np_reviews = np.array(reviews)\n",
        "np_labels = np.array(labels)\n",
        "for train_index, test_index in kf.split(np_reviews):\n",
        "    X_train, X_test = np_reviews[train_index], np_reviews[test_index]\n",
        "    y_train, y_test = np_labels[train_index], np_labels[test_index]\n",
        "    model = Pipeline([('vect', CountVectorizer(ngram_range=(1,1),\n",
        "                                             max_df=0.8,\n",
        "                                             max_features=None)), \n",
        "                     ('tfidf', TfidfTransformer()), \n",
        "                     ('clf', MultinomialNB())\n",
        "                    ])\n",
        "    model.fit(X_train, y_train)\n",
        "    scores.append(model.score(X_test, y_test))\n",
        "print(scores)\n",
        "print(np.mean(scores))\n",
        "final_model = Pipeline([('vect', CountVectorizer(ngram_range=(1,1),\n",
        "                                             max_df=0.8,\n",
        "                                             max_features=None)), \n",
        "                     ('tfidf', TfidfTransformer()), \n",
        "                     ('clf', MultinomialNB())\n",
        "                    ])\n",
        "#final_model.fit(X, y)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.6115217391304347, 0.5960869565217392, 0.596304347826087, 0.6065217391304348, 0.5979560774081322]\n",
            "0.6016781720033656\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mLOG2NGjF2Cn"
      },
      "source": [
        "### Create result label file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fF__Gpg_FhKK"
      },
      "source": [
        "#using final model to predict\n",
        "y_pred = final_model.predict(tests)\n",
        "#create result label file\n",
        "filename = '/data/LR_res.txt'\n",
        "f = open(filename, \"w\", encoding=\"utf8\")\n",
        "for line in y_pred:\n",
        "    f.write(line + \"\\n\")\n",
        "f.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aYRrM600rnVW"
      },
      "source": [
        "###Fasttext"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_emLkS7nE4xO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d7386d85-da33-4ff2-83bf-19ceb04a9460"
      },
      "source": [
        "#fasttext\n",
        "!pip3 install numpy scipy pybind11 cmake\n",
        "!wget -nc \"https://github.com/facebookresearch/fastText/archive/v0.9.2.zip\" > /dev/null\n",
        "!unzip -n \"v0.9.2.zip\" > /dev/null\n",
        "!cd fastText-0.9.2 && pip3 install .\n",
        "!rm -rf v0.9.2.zip fastText-0.9.2"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (1.18.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (1.4.1)\n",
            "Collecting pybind11\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/00/84/fc9dc13ee536ba5e6b8fd10ce368fea5b738fe394c3b296cde7c9b144a92/pybind11-2.6.1-py2.py3-none-any.whl (188kB)\n",
            "\u001b[K     |████████████████████████████████| 194kB 5.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: cmake in /usr/local/lib/python3.6/dist-packages (3.12.0)\n",
            "Installing collected packages: pybind11\n",
            "Successfully installed pybind11-2.6.1\n",
            "--2020-12-06 15:32:59--  https://github.com/facebookresearch/fastText/archive/v0.9.2.zip\n",
            "Resolving github.com (github.com)... 140.82.114.3\n",
            "Connecting to github.com (github.com)|140.82.114.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://codeload.github.com/facebookresearch/fastText/zip/v0.9.2 [following]\n",
            "--2020-12-06 15:33:00--  https://codeload.github.com/facebookresearch/fastText/zip/v0.9.2\n",
            "Resolving codeload.github.com (codeload.github.com)... 140.82.113.9\n",
            "Connecting to codeload.github.com (codeload.github.com)|140.82.113.9|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [application/zip]\n",
            "Saving to: ‘v0.9.2.zip’\n",
            "\n",
            "v0.9.2.zip              [  <=>               ]   4.17M  16.6MB/s    in 0.3s    \n",
            "\n",
            "2020-12-06 15:33:00 (16.6 MB/s) - ‘v0.9.2.zip’ saved [4369852]\n",
            "\n",
            "Processing /content/fastText-0.9.2\n",
            "Requirement already satisfied: pybind11>=2.2 in /usr/local/lib/python3.6/dist-packages (from fasttext==0.9.2) (2.6.1)\n",
            "Requirement already satisfied: setuptools>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from fasttext==0.9.2) (50.3.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from fasttext==0.9.2) (1.18.5)\n",
            "Building wheels for collected packages: fasttext\n",
            "  Building wheel for fasttext (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fasttext: filename=fasttext-0.9.2-cp36-cp36m-linux_x86_64.whl size=3040886 sha256=10387304839c02cc24b38dbfe3b57d97528c5ab521e2a602218f6565da0875ee\n",
            "  Stored in directory: /root/.cache/pip/wheels/1a/f3/f7/6e23c787ef7a44142867cdb80e5718cbbeb69e379e8e60e380\n",
            "Successfully built fasttext\n",
            "Installing collected packages: fasttext\n",
            "Successfully installed fasttext-0.9.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZLkRMqE-FmOR"
      },
      "source": [
        "#create train and test file for fasttext\n",
        "filename = '/data/train_fasttext.txt'\n",
        "f = open(filename, \"w\")\n",
        "for i in range(len(X_train)):\n",
        "    f.write(y_train[i] + ' ' + X_train[i] + \"\\n\")\n",
        "f.close()\n",
        "filename = '/data/test_fasttext.txt'\n",
        "f = open(filename, \"w\")\n",
        "for i in range(len(X_test)):\n",
        "    f.write(y_test[i] + ' ' + X_test[i] + \"\\n\")\n",
        "f.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VJW3_wCaFNFA",
        "outputId": "ea7e6681-4d36-45db-8ea4-265cc9a8515f"
      },
      "source": [
        "import fasttext\n",
        " \n",
        "start_time = time.time()\n",
        "model = fasttext.train_supervised(\n",
        "                                input='/data/train_fasttext.txt',\n",
        "                                #input='/data/en_train_fin_lower.txt',\n",
        "                                dim=100,\n",
        "                                epoch=8,\n",
        "                                lr=0.1,\n",
        "                                wordNgrams=2,\n",
        "                                label='__label__',\n",
        "                                minCount=5\n",
        ")\n",
        " \n",
        "train_time = time.time() - start_time\n",
        "print('Done training Fasttext in', train_time, 'seconds.')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Done training Fasttext in 3.7627036571502686 seconds.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xRic4JdXFRC1",
        "outputId": "b1620f90-3570-4f90-c724-26c0d55fe405"
      },
      "source": [
        "def print_results(N, p, r):\n",
        "    print(\"Fasttext, Precision = {}, Recall = {}\".format(p, r))\n",
        "\n",
        "print_results(*model.test('/data/test_fasttext.txt'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fasttext, Precision = 0.7027614698847575, Recall = 0.7027614698847575\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WmPznLIDfA06",
        "outputId": "b79958b5-12e1-4f36-c175-8eafd002f36c"
      },
      "source": [
        "#kfold fastext\n",
        "%%time\n",
        "import fasttext\n",
        "\n",
        "np_reviews = np.array(reviews)\n",
        "np_labels = np.array(labels)\n",
        "scores = []\n",
        "kf = KFold(n_splits=5, shuffle=True)\n",
        "for train_index, test_index in kf.split(np_reviews):\n",
        "    X_train, X_test = np_reviews[train_index], np_reviews[test_index]\n",
        "    y_train, y_test = np_labels[train_index], np_labels[test_index]\n",
        "    #create train and test file for fasttext\n",
        "    filename = '/data/train_fasttext.txt'\n",
        "    f = open(filename, \"w\")\n",
        "    for i in range(len(X_train)):\n",
        "        f.write(y_train[i] + ' ' + X_train[i] + \"\\n\")\n",
        "    f.close()\n",
        "    filename = '/data/test_fasttext.txt'\n",
        "    f = open(filename, \"w\")\n",
        "    for i in range(len(X_test)):\n",
        "        f.write(y_test[i] + ' ' + X_test[i] + \"\\n\")\n",
        "    f.close()\n",
        "    model = fasttext.train_supervised(\n",
        "                                input='/data/train_fasttext.txt',\n",
        "                                #input='/data/en_train_fin_lower.txt',\n",
        "                                dim=100,\n",
        "                                epoch=8,\n",
        "                                lr=0.1,\n",
        "                                wordNgrams=2,\n",
        "                                label='__label__',\n",
        "                                minCount=5)\n",
        "    scores.append(model.test('/data/test_fasttext.txt')[1])\n",
        "print(scores)\n",
        "print(np.mean(scores))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.7215217391304348, 0.6941304347826087, 0.6965217391304348, 0.6984782608695652, 0.7040661013263753]\n",
            "0.7029436550478838\n",
            "CPU times: user 20.3 s, sys: 1.31 s, total: 21.6 s\n",
            "Wall time: 21.6 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_EjoOUKGr8JM"
      },
      "source": [
        "###Finally write fasttext predicts to file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gUEI8ervR4II"
      },
      "source": [
        "filename = '/data/fat_res_fin.txt'\n",
        "f = open(filename, \"w\", encoding=\"utf8\")\n",
        "f2 = open(\"/data/en_test_fin_lower.txt\", \"r\")\n",
        "lines = f2.readlines()\n",
        "#lines2 = lines[:10]\n",
        "for line in lines:\n",
        "    s = model.predict(line.strip()) \n",
        "    f.write(s[0][0] + \"\\n\")\n",
        "f.close()\n",
        "f2.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vfQEKwO-EKYs"
      },
      "source": [
        "###Translate en file to vi and tokenize"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QEmfSNI9EkXA",
        "outputId": "3053057b-fd59-4f93-c5d0-9529a79dd81c"
      },
      "source": [
        "#file train\n",
        "%%time\n",
        "from google_trans_new import google_translator\n",
        "f = open(\"/data/vi_train_from_en_tokenized.txt\", \"w\")\n",
        "translator = google_translator()\n",
        "filename = '/data/en_train_fin_lower.txt'\n",
        "f2 = open(filename, \"r\", encoding=\"utf8\")\n",
        "lines = f2.readlines()\n",
        "#lines2 = lines[:5]\n",
        "for line in lines:\n",
        "  l = line.strip().split(' ', 1)\n",
        "  f.write(l[0] + ' ' + word_tokenize(translator.translate(l[1],'vi').lower(), format = 'text') + \"\\n\")\n",
        "  #f2.write(translator.translate(line).lower() + \"\\n\")\n",
        "f2.close()\n",
        "f.close()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 2min 48s, sys: 8.29 s, total: 2min 56s\n",
            "Wall time: 59min 8s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tEj0QtDSEmF0",
        "outputId": "d744f61b-9c66-4a5b-9190-d2206b11be94"
      },
      "source": [
        "#file test\n",
        "%%time\n",
        "f = open(\"/data/vi_test_from_en_tokenized.txt\", \"a\")\n",
        "filename = '/data/en_test_fin_lower.txt'\n",
        "f2 = open(filename, \"r\", encoding=\"utf8\")\n",
        "lines = f2.readlines()\n",
        "lines2 = lines[9471:]\n",
        "for line in lines2:\n",
        "  f.write(word_tokenize(translator.translate(line,'vi').lower(), format = 'text') + \"\\n\")\n",
        "f2.close()\n",
        "f.close()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 1min 20s, sys: 3.48 s, total: 1min 23s\n",
            "Wall time: 28min 11s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kbTXqX9PGcXb"
      },
      "source": [
        "###GridSearch "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xxqB-xB7GkqV"
      },
      "source": [
        "#Todo"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}